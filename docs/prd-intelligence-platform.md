______________________________________________________________________

stepsCompleted: [1, 2, 3, 4, 6, 7, 8, 9, 10]
inputDocuments:

- docs/innovation-strategy-2025-12-09.md
- docs/product-brief-gitlab-insights-2025-11-19.md
- docs/prd.md
  documentCounts:
  briefs: 1
  research: 0
  brainstorming: 0
  projectDocs: 2
  innovationStrategy: 1
  workflowType: 'prd'
  lastStep: 8
  project_name: 'gitlab-insights'
  user_name: 'Taylor'
  date: '2025-12-12'

______________________________________________________________________

# Product Requirements Document - gitlab-insights

**Author:** Taylor
**Date:** 2025-12-12

______________________________________________________________________

## Executive Summary

// ... existing code ...

## Success Criteria

### User Success

Success is defined by **personal effectiveness enhancement** first, organizational adoption second. The intelligence platform succeeds when it makes YOU demonstrably more effective at organizational brokerage through high-value interventions.

**Primary Success Indicators:**

1. **Daily Active Usage:** You use GitLab Insights 6-7 days/week by Week 4, becoming habitual awareness mechanism
1. **Intervention Frequency:** 5+ high-value interventions per week by Week 9 (connecting engineers, preventing duplicates, surfacing decisions, unblocking work)
1. **Colleague Recognition:** 3+ colleagues explicitly ask "How did you know?" by Week 12, demonstrating visible effectiveness
1. **Organic Access Requests:** 3-5 colleagues request tool access (unsolicited) by Week 16
1. **Organizational Positioning:** Recognized as "person who knows what's happening across teams" by 3+ colleagues

**Intervention Success Patterns:**

- **Expertise Routing:** Successfully connect engineers to domain experts (10+ queries/week leading to successful routing)
- **Duplicate Prevention:** Catch 2+ near-collisions per week that warrant coordination
- **Decision Context:** Surface 5+ instances where historical context prevents re-litigation of already-decided topics
- **Proactive Unblocking:** Detect and address stalled high-priority work before it becomes critical

**Failure Signals:**

- Tool usage drops below 4 days/week (not valuable enough)
- Zero unsolicited "how did you know?" questions by Week 12 (interventions not notable)
- No organic access requests by Week 16 (value not compelling or not visible)

### Business Success

**Organizational Impact (Measured Through Interventions):**

**By Week 16:**

- **Cumulative Time Saved:** 200+ hours saved from interventions (prevented duplicates, faster expert routing, decision context, unblocking)
- **High-Impact Interventions:** 5+ instances preventing major waste (1+ week duplicate work, critical blockers, architecture misalignments)
- **Prevented Rework:** 3+ instances of conflicting work caught before merge/production

**Strategic Positioning Outcomes:**

- **Organizational Intelligence Hub:** You become go-to person for "who's working on what" and "who knows about X"
- **Career Capital:** Recognition in performance reviews for cross-team coordination and organizational awareness
- **Organizational Leverage:** Increased influence on cross-team decisions despite role/authority

**Conditional Scaling Paths:**

**Path A - High Adoption (15+ active users by Month 6):**

- Tool spreads via word-of-mouth as early adopters make interventions
- Management notices coordination improvements
- Positioned for official infrastructure adoption

**Path B - Moderate Adoption (5-15 active users):**

- Stable niche product for power users (senior engineers, cross-team contributors)
- Maintained as unofficial tool with low time investment

**Path C - Personal Utility (\<5 users):**

- High personal value to YOU
- Career capital from visible interventions
- Learning value (AI-assisted development, intelligence architecture)

**All paths succeed because minimum criteria = personal effectiveness enhancement.**

### Technical Success

**Performance Targets (Existing MVP + Intelligence Layer):**

- Page load performance: \<500ms (attention is precious)
- Search results: \<1s (expertise queries, decision archaeology)
- Expertise calculation: \<2s for full codebase analysis
- Territory radar updates: Real-time (\<5s latency)
- Activity summaries: Generated in \<3s

**Intelligence Feature Reliability:**

- Expertise Discovery: 70%+ accuracy (suggested experts actually knowledgeable)
- Decision Archaeology: 80%+ relevance (surfaced discussions actually related)
- Territory Radar: 90%+ collision detection (alerts represent real overlaps)
- Activity Summaries: 95%+ signal quality (filters noise effectively)

**Development Velocity (Phase 1 - Weeks 1-8):**

- Feature shipping cadence: 1 feature every 1-2 weeks
- Time investment: 10-20 hrs/week development
- AI-assisted development effectiveness: 80%+ code generated by AI requires minimal debugging

**Data Quality & Availability:**

- GitLab API data richness validated (contribution data, comment access, file history available)
- PostgreSQL schema supports expertise caching and graph queries
- Integration with existing event stream architecture

### Measurable Outcomes

**Week 8 Go/No-Go Decision:**

- ✅ **GO (proceed to Phase 2):** High personal usage + 15+ interventions + colleagues asking "how did you know?"
- ⚠️ **PIVOT:** Low usage but one feature valuable → Focus on that feature alone
- ❌ **KILL:** No personal value, tool feels like chore → Cut losses, learn from experiment

**Week 16 Go/No-Go Decision:**

- ✅ **GO (Phase 3 expansion):** Organic demand (3+ access requests) + early adopters engaged + continued personal value
- ⚠️ **KEEP PERSONAL:** High personal value but no demand → Keep as personal tool, don't scale
- ⚠️ **PIVOT:** Demand exists but different use case → Adjust to what users actually want
- ❌ **KILL:** No personal value + no demand → Major pivot or project termination

**Leading Indicators (Track Weekly):**

1. Personal usage days (target: 6-7/week)
1. Non-obvious insights discovered per session (target: 3+/session)
1. Weekly intervention count (target: 2+/week Phase 1, 5+/week Phase 2)
1. Intervention acceptance rate (target: 50%+ Week 10, 60%+ Week 16)
1. Colleague curiosity questions (target: 1+/week after Week 9)

**Lagging Indicators (Track Monthly):**

1. Cumulative time saved from interventions
1. High-impact interventions count
1. Organizational recognition qualitative assessment
1. Career capital accumulation (performance review mentions, scope expansion)

______________________________________________________________________

## Product Scope

### MVP - Phase 1: Core Intelligence Build (Weeks 1-8)

**The MVP is four standalone intelligence features** that enable personal brokerage capability. Built using clever GitLab API queries and smart data presentation - no ML required.

**1. Expertise Discovery Engine (Weeks 1-2)**

- GitLab API queries for contribution patterns (commits, issues, MRs by user + file path)
- "Who knows about X?" search interface with ranked contributors
- Expertise profiles: "Sarah: 47 commits in auth/\*, 12 issues closed"
- Expertise heatmap visualization (codebase tree colored by contributor concentration)
- Algorithm: Score = (commit_count × 3) + (issue_count × 2) + (mr_count × 4) with recency weighting
- **Success:** 10+ expertise queries in week 2 leading to successful routing

**2. Decision Archaeology System (Weeks 3-4)**

- Enhanced full-text search (PostgreSQL FTS) with decision-focused features
- Timeline view showing evolution of discussions
- Related discussions grouping (keyword overlap, shared participants)
- Simple heuristic decision extraction (keywords: "decided," "conclusion," "we'll go with")
- **Success:** 5+ instances where historical context prevents re-litigation

**3. Code Territory Radar (Weeks 5-6)**

- Track open MRs + recent commits by file path (real-time event stream)
- "Who's working where" visualization by file/module
- Collision detection: IF two+ users have open MRs touching same file → Alert
- Historical territory: Calculate primary maintainer (most commits in last 6 months)
- **Success:** 2+ near-collisions detected per week warranting coordination

**4. Generated Activity Summaries (Weeks 7-8)**

- Aggregate GitLab events by time window + person/team
- Daily digest: "Alex: 3 commits to auth, 2 issues closed, 1 MR opened"
- Stalled work detection: High-priority issues with no activity 3+ days
- Weekly rollup with activity trends (commit count, issue velocity, review participation)
- **Success:** Daily usage as primary awareness mechanism, 3+ proactive unblocking interventions

**Critical MVP Requirements:**

- Integrate with existing GitLab API polling infrastructure
- Maintain \<500ms page load, \<1s search performance
- Use existing PostgreSQL schema with new tables for expertise/territory caching
- Consistent with existing keyboard-first UI patterns
- No ML/LLM dependencies - pure data aggregation and smart presentation

### Growth Features - Phase 2: Advanced Intelligence (Post-Week 8 Validation)

**If Phase 1 proves valuable (Week 8 go decision), add:**

**Duplicate Work Detection (Weeks 9-11)**

- Text similarity algorithms (TF-IDF or basic embedding-based matching)
- "Someone else is working on similar functionality" alerts
- Cross-team duplicate identification before effort wasted

**Interest-Based Conversation Discovery (Weeks 12-14)**

- Implicit interest learning from code contributions + engagement history
- Explicit interest controls (manual topic/area specification)
- "Discussions you might be interested in" feed for breaking notification bubbles

**Context-Aware Work Assistant (Weeks 15-17)**

- "I'm working on bug #789" → System surfaces expertise, past discussions, common pitfalls, related work
- Combines expertise queries + full-text search + file path analysis on current work context
- Sidebar integration for context without leaving workflow

### Vision - Phase 3: Organic Expansion (Conditional, Weeks 17+)

**If organic demand emerges (5+ users, word-of-mouth spread):**

**Multi-User Network Effects:**

- Shared decision tagging improves archaeology for all users
- Collaborative watchlists and team views
- Expertise graphs improve with more contribution data

**Integration Ecosystem:**

- IDE integration: VS Code sidebar showing experts and historical context
- GitLab UI browser extension: Augment GitLab pages with intelligence data

**Advanced Pattern Recognition:**

- Predict team collisions before they occur (trend analysis on territory data)
- Detect duplicate work via semantic similarity
- Recommend reviewers based on expertise + code territory

**Organizational Intelligence Features:**

- Knowledge graph: "Who knows what" mapping with relationship strength
- Institutional memory preservation: Link decisions to documentation
- Onboarding acceleration: New hire views showing team context

**Official Infrastructure Path (If 15+ active users):**

- Production-grade security/auth improvements
- SLA commitment and monitoring
- Hand-off documentation for team maintenance
- Privacy policy and usage transparency

______________________________________________________________________

## User Journeys

**Journey 1: Sarah Chen - Finding the Authentication Expert**

Sarah is a senior backend engineer tasked with adding OAuth2 support to the API. She's never touched the authentication layer before, and the codebase has no clear ownership documentation. She starts by searching the GitLab repository for "auth" - 847 results across 12 projects. She tries asking in the general engineering channel: "Anyone know who owns the auth system?" Three people respond with different names, none of them certain.

Frustrated, Sarah opens GitLab Insights and searches "auth authentication" in the Expertise Discovery. Within seconds, she sees Jordan Martinez: 68 commits in auth/\*, 15 issues closed, last contribution 3 days ago. The expertise heatmap shows Jordan owns 72% of commits in the authentication module over the past 6 months. Sarah reaches out to Jordan directly with her specific question.

Jordan responds within an hour with exactly the context Sarah needed - not just answering her question, but pointing her to the critical design decisions in issue #1247 and warning her about the edge case that caused problems last time. What would have been days of trial-and-error becomes a focused 4-hour implementation. Sarah successfully ships OAuth2 support without introducing bugs or duplicating work Jordan had already attempted.

**Journey 2: Alex Rivera - Uncovering the API Design Rationale**

Alex is debugging a production issue where the API returns 200 OK even when operations partially fail. This seems wrong, but the code has been this way for two years. Before proposing a fix, Alex wants to understand if this was intentional. He searches GitLab for "API error handling" and finds 200+ issues and MRs, but no clear answer about why this pattern exists.

Alex opens Decision Archaeology and searches "API partial failure response codes". The timeline view surfaces a critical discussion from 18 months ago in issue #892. The thread shows the team deliberately chose this pattern because their main client (a mobile app) had unreliable network conditions and needed to know which items succeeded even if others failed. The resolution comment explicitly states: "200 with error array in response body - allows client to retry only failed items."

Armed with this context, Alex realizes his "fix" would break the mobile app's retry logic. Instead of creating a breaking change, he proposes a new endpoint version that handles this use case properly while maintaining backward compatibility. The decision that seemed like a bug was actually a carefully considered trade-off - and knowing the "why" prevents Alex from undoing two years of client integration work.

**Journey 3: Morgan Lee - Avoiding a Collision**

Morgan picks up a high-priority story to refactor the payment processing module for better error handling. She creates a feature branch and starts making changes to `payments/core.ts`. After two days of work, she's ready to create an MR when Code Territory Radar shows an alert: "Riley Thompson has an open MR modifying payments/core.ts (MR #1453 - opened 5 days ago)."

Morgan checks Riley's MR and discovers they're both refactoring the same error handling logic, but with different approaches. Instead of continuing in parallel, Morgan sends Riley a quick message: "Hey, saw we're both in payments/core - want to sync up?" They hop on a 15-minute call and realize Riley's approach is more comprehensive and further along. Morgan shifts her focus to testing Riley's changes and adding edge cases they discuss together.

What could have been a painful merge conflict and days of reconciling two incompatible refactors becomes a collaboration. Riley's MR ships two days later with Morgan's test improvements, and Morgan moves on to the next high-priority item instead of duplicating effort. The territory radar saved both engineers from wasted work and prevented a messy production rollback.

**Journey 4: Jordan Martinez - Discovering the Stalled Security Issue**

Jordan opens GitLab Insights on Monday morning and scans the Activity Summaries. The daily digest shows normal activity across most teams, but the stalled work detection flags a critical item: "Issue #2103 (Priority: High, Label: security) - No activity for 5 days. Assigned to: Chris Park."

Jordan knows Chris is usually responsive, so 5 days of silence is unusual for a security issue. Jordan checks the issue and sees Chris asked a question about GDPR compliance implications but never got a response. Jordan happens to know that Elena from the legal team handles this - he makes an introduction, and Elena clarifies the compliance requirements within an hour.

Chris unblocks and ships the security fix that afternoon. Without the activity summary surfacing the stalled work, this high-priority security issue could have sat untouched for weeks while everyone assumed someone else was handling it. The 2-minute intervention prevented what could have been a compliance violation or security incident.

**Journey 5: Priya Patel - Onboarding to an Unfamiliar Codebase**

Priya just joined the team and is assigned her first task: add a new notification type to the event system. She's never worked in this codebase before and doesn't know where to start. Reading through the code, she has questions: Why does this use a polling pattern instead of webhooks? What's the performance impact of adding another event type?

Priya searches Decision Archaeology for "event system notification architecture" and finds a comprehensive discussion from 8 months ago (issue #756). The thread reveals the team originally tried webhooks but GitLab's webhook delivery had reliability issues at scale. They switched to polling with aggressive caching, and the comments include specific performance benchmarks showing the trade-offs.

She also uses Expertise Discovery to find that Taylor implemented the current event system (45 commits in events/\*). A quick conversation with Taylor gives her the context she needs about edge cases and testing strategies. What could have taken a week of reading code and making wrong assumptions becomes a focused 2-day feature implementation. Priya's first contribution follows existing patterns and avoids the architectural mistakes the team already learned from.

### Journey Requirements Summary

These journeys reveal the following capability requirements:

**Expertise Discovery Engine:**

- Search by file path, module, or topic keywords
- Contribution scoring with recency weighting
- Expertise heatmap visualization showing code ownership concentration
- Last contribution timestamp to verify expert is still active

**Decision Archaeology System:**

- Full-text search across issues and MR discussions
- Timeline view showing discussion evolution
- Resolution/decision extraction and highlighting
- Related discussion grouping (same topic, different time periods)

**Code Territory Radar:**

- Real-time tracking of open MRs by file path
- Collision detection for overlapping work
- Historical territory ownership calculation
- Alert system for potential conflicts

**Activity Summaries:**

- Daily digest of commits, issues, and MR activity by person
- Stalled work detection for high-priority items
- Activity gap identification (no updates for N days)
- Team-level rollup views

**Cross-cutting Requirements:**

- \<1s search performance (engineers won't wait)
- Keyboard-first navigation (engineers prefer keyboard)
- Integration with existing GitLab authentication
- Multi-project support (engineers work across repos)

______________________________________________________________________

## Innovation & Novel Patterns

### Detected Innovation Areas

**Business Model Innovation: The Trojan Horse Intelligence Platform**

Traditional internal tool adoption follows a predictable pattern: build → market internally → drive adoption → hope for value creation. gitlab-insights intelligence platform inverts this entirely. The innovation is making the tool invisible while your enhanced effectiveness becomes the product. Colleagues don't see a dashboard demo - they experience impossibly well-informed interventions and naturally ask "How did you know that?"

This approach challenges the fundamental assumption that internal tools succeed through product features and user adoption metrics. Instead, success is measured by personal leverage first (you becoming organizational intelligence hub), with adoption as optional upside. The tool serves as infrastructure for creating information asymmetry in siloed environments, not as a self-service product.

**Strategic Positioning Innovation: Conditional Scaling with Preserved Upside**

The three-path model (personal utility → moderate adoption → high adoption) represents a novel approach to managing internal tool risk. All paths constitute success because the minimum viable outcome - personal effectiveness enhancement - delivers value regardless of organizational adoption. This preserves optionality without requiring commitment to a single scaling strategy.

**Technical Architecture Innovation: Intelligence Without ML**

In an era where "intelligence platforms" typically mean ML models and LLM integration, this platform achieves 80% of value through clever GitLab API queries and smart data presentation. The innovation is recognizing that for organizational intelligence, graph patterns and aggregation deliver more reliable insights than probabilistic ML approaches - especially for a stealth tool where reliability outweighs sophistication.

### Market Context & Competitive Landscape

**Existing Category: Engineering Analytics Platforms**

Tools like LinearB, Swarmia, and Pluralsight Flow provide engineering metrics and productivity analytics. However, these are management-focused dashboards designed for visibility into team performance, not individual engineer empowerment for brokerage.

**Differentiation:**

1. **User**: Individual engineers (especially you) vs. engineering managers
1. **Goal**: Organizational influence through information asymmetry vs. team productivity metrics
1. **Adoption**: Stealth/organic vs. top-down management mandate
1. **Value Delivery**: Personal interventions vs. dashboard insights
1. **Scaling Strategy**: Conditional (success without adoption) vs. required (only valuable at scale)

**Innovation Gap**: No existing tool combines expertise discovery, decision archaeology, territory awareness, and activity intelligence specifically for individual engineer leverage in siloed organizations. The intelligence is designed for proactive coordination, not passive reporting.

### Validation Approach

**Phase 1 Validation (Weeks 1-8): Personal Effectiveness**

The innovation's first validation is whether it makes YOU measurably more effective:

- Success criteria: 5+ high-value interventions per week using tool insights
- Validation method: Document interventions and track whether they prevented real waste/conflict
- Failure signal: Tool usage drops below 4 days/week (not valuable enough)
- Decision point: Week 8 go/no-go based on personal value alone

**Phase 2 Validation (Weeks 9-16): Visible Impact**

Validate whether enhanced effectiveness creates organic curiosity:

- Success criteria: 3+ colleagues ask "How did you know?" without prompting
- Validation method: Track unsolicited questions and access requests
- Failure signal: Zero organic interest by Week 12 (interventions not notable)
- Decision point: Week 16 determines scaling path (personal, moderate, high adoption)

**Business Model Validation: Does Value-Through-Behavior Work?**

The core innovation hypothesis is that demonstrating value through interventions creates stronger pull demand than product demos. Validation requires:

- Colleagues attributing coordination improvements to your awareness (not luck)
- Access requests coming from observation of your effectiveness (not you offering)
- Word-of-mouth spread ("You should ask Taylor about that tool")

If colleagues don't ask how you know things by Week 12, the Trojan Horse model failed - the tool may still have personal utility, but the innovative adoption strategy didn't work.

### Risk Mitigation

**Innovation Risk 1: Brokerage Role Doesn't Create Demand**

**Risk**: Colleagues view interventions as helpful but don't connect them to tool capability, so no organic adoption occurs.

**Mitigation**:

- Selectively reveal the tool when asked "how did you know?" (don't hide indefinitely)
- Document high-value interventions to prove tool's role if demand doesn't materialize
- Fallback: Keep as personal tool (still delivers value via Path C)

**Innovation Risk 2: Information Asymmetry Creates Negative Perception**

**Risk**: Being "too aware" of what everyone's working on feels invasive or creates political backlash.

**Mitigation**:

- Frame interventions as helpful coordination, not surveillance
- Only surface publicly available GitLab data (no private information)
- If concerns arise, emphasize transparency: "It's just GitLab API queries, nothing hidden"
- Fallback: Reduce intervention frequency, use tool passively for personal awareness only

**Innovation Risk 3: Stealth Approach Limits Network Effects**

**Risk**: Waiting for organic demand means missing the window where early network effects would improve intelligence quality.

**Mitigation**:

- Week 16 decision point allows pivot to active evangelism (Option B) if personal value proven but demand slow
- Multi-user features designed for Phase 3 preserve ability to scale if demand emerges
- Fallback: Accept Path B (moderate adoption) as successful outcome

**Innovation Risk 4: Simple Heuristics Insufficient for Intelligence**

**Risk**: Non-ML approach doesn't surface valuable enough insights, requiring pivot to ML/LLM features earlier than planned.

**Mitigation**:

- Week 8 validation specifically tests whether simple features deliver high-value interventions
- If insufficient, hypothesis 6 (Simple Features Beat Complex ML) fails → Add Phase 2 advanced features sooner
- LLM integration remains available as Phase 3 option if basic intelligence proves inadequate

______________________________________________________________________

## Web App Specific Requirements

### Browser Support Matrix

**Target Browsers:**

- Chrome/Edge (Chromium): Latest 2 versions
- Firefox: Latest 2 versions
- Safari: Latest 2 versions

**Rationale:** Internal tool for engineering team. No legacy browser support needed. Engineers use modern browsers exclusively.

**JavaScript Requirements:**

- ES2020+ features supported
- No IE polyfills required
- Modern async/await, fetch API, WebSocket support assumed

### Responsive Design & Display Targets

**Primary Display Target:** Desktop/laptop (1920x1080 and above)

**Rationale:** Engineering intelligence platform optimized for developer workstations. Keyboard-driven navigation assumes full keyboard availability. Mobile usage not a primary scenario - engineers use this at their desks during active development.

**Responsive Breakpoints:**

- Desktop: 1920px+ (primary)
- Laptop: 1366px-1919px (secondary)
- Tablet/Mobile: Not optimized (functional but not primary UX focus)

### Performance Targets

**Page Load Performance:**

- Initial page load: \<500ms (attention is precious)
- Route transitions: \<200ms (SPA navigation)
- Search/query results: \<1s (expertise, decision archaeology)
- Complex calculations: \<2s (expertise scoring across full codebase)
- Real-time updates: \<5s latency (territory radar, activity summaries)

**Performance Budget:**

- JavaScript bundle: \<300KB gzipped
- Initial CSS: \<50KB gzipped
- Time to interactive: \<1s on cable connection
- Lighthouse performance score: >90

**Caching Strategy:**

- Aggressive caching of GitLab data (expertise profiles, contribution graphs)
- Real-time invalidation via event stream
- Stale-while-revalidate pattern for non-critical data

### SEO Strategy

**SEO Requirements:** None - authentication-required internal tool

**Indexing:**

- robots.txt: Disallow all
- No sitemap needed
- No structured data markup
- No social media meta tags

**Rationale:** Tool not discoverable via search engines. Access granted directly to authenticated users only.

### Accessibility Level

**Target Compliance:** WCAG 2.1 Level AA practices (not formally certified)

**Keyboard Navigation:**

- Primary input method (keyboard-first design philosophy)
- All features accessible via keyboard shortcuts
- Tab navigation follows logical flow
- Focus indicators clearly visible
- No mouse-required interactions

**Screen Reader Support:**

- Semantic HTML structure
- ARIA labels for dynamic content
- Announced status updates (collision alerts, stalled work notifications)
- Skip links for main content areas

**Visual Accessibility:**

- Minimum contrast ratios met (4.5:1 for normal text)
- Resizable text without layout breaking
- No color-only information conveyance
- Reduced motion support for animations

**Rationale:** Engineering audience values keyboard efficiency. Accessibility features support diverse engineer needs and improve overall UX quality.

### Real-Time Requirements

**Real-Time Features:**

- Code Territory Radar: Live tracking of open MRs and recent commits
- Activity Summaries: Near real-time event aggregation
- Collision Alerts: Immediate notification when overlapping work detected

**Technology Approach:**

- WebSocket connection for real-time updates
- Fallback to polling if WebSocket unavailable
- Event stream from existing GitLab API polling infrastructure
- Client-side state reconciliation

**Update Frequency:**

- Territory changes: \<5s latency
- Activity summaries: 1-minute batch updates
- Expertise recalculation: On-demand + nightly batch

### Integration Points

**GitLab API Integration:**

- OAuth authentication (existing MVP pattern)
- REST API for queries (commits, issues, MRs)
- GraphQL for complex graph queries (contribution patterns)
- Webhook fallback if real-time polling insufficient

**Existing MVP Integration:**

- Reuse event capture infrastructure
- Share PostgreSQL database and caching layer
- Consistent UI patterns and navigation
- Same authentication and authorization model

### Technical Constraints

**Must Maintain:**

- \<500ms page load performance (existing MVP standard)
- Keyboard-first navigation patterns
- Existing PostgreSQL schema compatibility
- OAuth authentication flow

**Cannot Use:**

- Server-side rendering for SEO (not needed, adds complexity)
- Mobile-specific views (not primary use case)
- ML/LLM processing (Phase 1 constraint - simple heuristics only)
- Real-time collaboration features (single-user focused initially)

______________________________________________________________________

## Project Scoping & Phased Development

### MVP Strategy & Philosophy

**Selected Approach: Problem-Solving MVP with Validation Gates**

This project uses a problem-solving MVP approach with conditional scaling paths. The MVP is four standalone intelligence features that enable personal organizational brokerage capability. Success is defined by personal effectiveness enhancement first, with organizational adoption as optional upside.

**Strategic Rationale:**

The Trojan Horse adoption strategy requires the tool to be invisible while your enhanced effectiveness becomes the product. This means the MVP must deliver genuine value to YOU personally, independent of whether anyone else uses it. The validation-gated approach (Week 8 and Week 16 decision points) ensures we don't over-invest before proving value.

**Core Philosophy:**

1. **Personal utility is minimum viable success** - If the tool makes YOU demonstrably more effective at organizational brokerage, it succeeds regardless of adoption
2. **Validation before expansion** - Each phase requires proof of value before proceeding
3. **Conditional scaling with preserved upside** - Three success paths (personal → moderate → high adoption) all constitute wins
4. **Simple features beat complex ML** - 80% of intelligence value from clever queries and smart presentation, not ML models
5. **Stealth by design** - Value demonstrated through interventions, not product demos

### MVP Boundaries & Exclusions

**In Scope for Phase 1 MVP (Weeks 1-8):**

1. **Expertise Discovery Engine** (Weeks 1-2)
   - GitLab API contribution pattern analysis
   - "Who knows about X?" search interface with ranked contributors
   - Expertise profiles and heatmap visualization
   - Simple scoring algorithm: `(commit_count × 3) + (issue_count × 2) + (mr_count × 4)` with recency weighting

2. **Decision Archaeology System** (Weeks 3-4)
   - Full-text search across issues and MR discussions (PostgreSQL FTS)
   - Timeline view showing discussion evolution
   - Simple heuristic decision extraction (keywords: "decided," "conclusion," "we'll go with")
   - Related discussions grouping via keyword overlap

3. **Code Territory Radar** (Weeks 5-6)
   - Real-time tracking of open MRs + recent commits by file path
   - "Who's working where" visualization
   - Collision detection: IF two+ users have open MRs touching same file → Alert
   - Historical territory calculation (primary maintainer = most commits in last 6 months)

4. **Generated Activity Summaries** (Weeks 7-8)
   - Daily digest aggregating GitLab events by person/team
   - Stalled work detection (high-priority issues with no activity 3+ days)
   - Weekly rollup with activity trends
   - Integration with existing event stream

**Explicitly Out of Scope for MVP:**

- **Duplicate Work Detection** (Phase 2) - Requires text similarity algorithms, deferred until personal value proven
- **Interest-Based Discovery** (Phase 2) - Implicit learning and recommendation engine, not needed for initial brokerage
- **Context-Aware Work Assistant** (Phase 2) - Sidebar integration and context combination, advanced feature
- **LLM-powered features** (Excluded from Phase 1) - Simple heuristics sufficient for initial intelligence
- **Multi-user collaboration features** (Phase 3) - Shared tagging, collaborative watchlists require network effects
- **IDE integration** (Phase 3) - VS Code sidebar requires proven demand
- **Browser extensions** (Phase 3) - GitLab UI augmentation deferred until adoption validated
- **Advanced pattern recognition** (Phase 3) - Collision prediction, semantic similarity, reviewer recommendation all require ML

**Critical Constraints:**

- No ML/LLM dependencies in Phase 1 - pure data aggregation and smart presentation
- No multi-project architecture initially - focus on single GitLab instance
- No mobile optimization - desktop/laptop only for engineering audience
- No real-time collaboration - single-user focus initially
- No public API - internal tool only

### Feature Prioritization Matrix

**Must-Have (Required for Week 8 validation):**

| Feature | Rationale | Risk if Excluded |
|---------|-----------|------------------|
| Expertise Discovery Engine | Core brokerage capability - "who knows X?" is #1 coordination need | Cannot route expertise effectively, interventions impossible |
| Code Territory Radar | Prevents duplicate work and merge conflicts - high-value interventions | Miss collision detection opportunities, waste not prevented |
| Activity Summaries | Awareness mechanism - daily usage driver and stalled work detection | No habitual usage pattern, proactive unblocking impossible |
| GitLab OAuth Integration | Security and data access requirement | Cannot access contribution data, tool non-functional |
| PostgreSQL Caching | Performance requirement (\<1s search, \<2s expertise calculation) | Queries too slow, user abandonment |
| Keyboard Navigation | Primary input method for developer audience | Poor UX for target users, reduced usage |

**Should-Have (Improves MVP but not required for validation):**

| Feature | Rationale | Workaround if Excluded |
|---------|-----------|------------------------|
| Decision Archaeology | Surfaces historical context - high-value but not daily need | Manual GitLab search fallback (slower, less insight) |
| Expertise Heatmap Visualization | Makes ownership patterns obvious - better UX than tables | Text-based expertise profiles sufficient |
| Related Discussions Grouping | Connects fragmented context - improves archaeology value | Manual cross-referencing by user |
| Weekly Activity Rollups | Trend identification for strategic awareness | Focus on daily digests only |

**Could-Have (Nice to have, defer if time-constrained):**

| Feature | Rationale | Defer Strategy |
|---------|-----------|----------------|
| Historical Territory View | Shows code ownership evolution over time | Initial version: last 6 months only |
| Custom Search Filters | Advanced query options (date range, author, project) | Start with simple keyword search |
| Export/Share Interventions | Document value created for later evangelism | Manual notes sufficient for validation |
| Browser Notifications | Push alerts for collision detection | In-app notifications sufficient initially |

**Won't-Have (Explicitly excluded from all phases unless demand proven):**

- Slack/IM app integration
- Mobile-specific responsive views
- Public documentation or marketing site
- Multi-tenant architecture
- SAML/SSO authentication (OAuth sufficient)

### Development Phases & Milestones

**Phase 1: Core Intelligence Build (Weeks 1-8)**

**Milestone 1: Expertise Discovery (Weeks 1-2)**
- Deliverables: "Who knows X?" search, expertise profiles, contribution scoring
- Success criteria: 10+ expertise queries in week 2 leading to successful routing
- Decision point: Is expertise data accurate enough for brokerage? If no, fix algorithm before continuing

**Milestone 2: Decision Archaeology (Weeks 3-4)**
- Deliverables: Full-text search, timeline view, decision extraction, related discussions
- Success criteria: 5+ instances where historical context prevents re-litigation
- Decision point: Is full-text search performance acceptable (\<1s)? If no, optimize before continuing

**Milestone 3: Code Territory Radar (Weeks 5-6)**
- Deliverables: Real-time MR tracking, collision detection, territory visualization
- Success criteria: 2+ near-collisions detected per week warranting coordination
- Decision point: Does real-time update latency meet \<5s requirement? If no, investigate WebSocket issues

**Milestone 4: Activity Summaries (Weeks 7-8)**
- Deliverables: Daily digests, stalled work detection, weekly rollups
- Success criteria: Daily usage as primary awareness mechanism, 3+ proactive unblocking interventions
- Decision point: Week 8 go/no-go validation

**Week 8 Go/No-Go Decision:**

✅ **GO (proceed to Phase 2):**
- High personal usage (6-7 days/week)
- 15+ high-value interventions completed
- Colleagues asking "how did you know?" (recognition of effectiveness)
- At least 2 of 4 features delivering measurable value

⚠️ **PIVOT:**
- Low usage but one feature highly valuable → Focus on that feature alone, kill others
- Features work but interventions not valuable → Reassess brokerage role hypothesis
- Technical blockers (performance, data quality) → Fix infrastructure before expanding

❌ **KILL:**
- No personal value, tool feels like chore → Cut losses, document learnings
- GitLab API limitations prevent core functionality → Abandon platform approach
- Time investment exceeds 20 hrs/week with no value → Unsustainable, terminate

**Phase 2: Advanced Intelligence (Weeks 9-17, conditional on Week 8 GO)**

**Milestone 5: Duplicate Work Detection (Weeks 9-11)**
- Deliverables: Text similarity algorithms, cross-team duplicate alerts
- Success criteria: 2+ duplicates caught before significant effort wasted
- Validation: Does duplicate detection find real overlaps or false positives?

**Milestone 6: Interest-Based Discovery (Weeks 12-14)**
- Deliverables: Implicit interest learning, "discussions you might care about" feed
- Success criteria: 10+ relevant discussions surfaced per week that you weren't tracking
- Validation: Does this break you out of notification bubbles or add noise?

**Milestone 7: Context-Aware Assistant (Weeks 15-17)**
- Deliverables: "I'm working on X" → expertise, discussions, pitfalls surfaced
- Success criteria: Used 5+ times per week, context saves 30+ minutes per use
- Decision point: Week 16 go/no-go validation

**Week 16 Go/No-Go Decision:**

✅ **GO (Phase 3 expansion):**
- Organic demand: 3+ unsolicited access requests
- Early adopters actively engaged (2+ colleagues using weekly)
- Continued high personal value (interventions ongoing)
- Word-of-mouth spread happening naturally

⚠️ **KEEP PERSONAL:**
- High personal value but no organic demand → Maintain as personal tool, don't scale
- Colleagues interested but not enough for network effects → Moderate adoption path

⚠️ **PIVOT:**
- Demand exists but for different use case → Adjust to what users actually want
- Adoption stalled due to specific barrier → Address barrier or accept limited audience

❌ **KILL:**
- No personal value by Week 16 despite features → Major pivot or project termination
- No demand and diminishing personal utility → Cut losses, focus elsewhere

**Phase 3: Organic Expansion (Weeks 17+, conditional on Week 16 demand signal)**

**Milestone 8: Multi-User Network Effects (Weeks 17-20)**
- Deliverables: Shared decision tagging, collaborative watchlists, team views
- Success criteria: 5+ active users, network effects measurably improving intelligence

**Milestone 9: Integration Ecosystem (Weeks 21-24)**
- Deliverables: VS Code sidebar, GitLab UI browser extension
- Success criteria: 10+ users, IDE integration requested organically

**Milestone 10: Official Infrastructure (Conditional on 15+ active users)**
- Deliverables: Production-grade auth, SLA monitoring, hand-off documentation
- Success criteria: Management recognizes value, willing to support officially

### Risk-Based Scoping Decisions

**Risk 1: Simple Heuristics Insufficient for Intelligence**

**Mitigation Strategy:**
- Week 8 validation specifically tests whether simple features deliver high-value interventions
- If insufficient, hypothesis 6 (Simple Features Beat Complex ML) fails
- Contingency: Add Phase 2 advanced features (text similarity, semantic search) sooner
- Fallback: Investigate LLM integration for decision extraction if keyword approach fails

**Scoping Decision:** Start with simplest possible algorithms (contribution counting, keyword matching). Only add complexity if validation proves simple insufficient.

**Risk 2: GitLab API Limitations Prevent Core Functionality**

**Mitigation Strategy:**
- Week 1-2 validate that GitLab API provides sufficient contribution data (commits, issues, MRs by file path)
- Test API rate limits and latency before building complex queries
- Contingency: If GraphQL insufficient, use REST API batch queries with caching
- Fallback: If real-time updates impossible, use polling with aggressive cache invalidation

**Scoping Decision:** Build expertise discovery first (API validation exercise). If API blockers discovered, reassess entire approach before investing in other features.

**Risk 3: Brokerage Role Doesn't Create Organic Demand**

**Mitigation Strategy:**
- Week 16 decision point allows pivot to active evangelism if personal value proven but demand slow
- Document high-value interventions to prove tool's role if demand doesn't materialize
- Contingency: If no demand by Week 16 but high personal value, accept Path C (personal utility) as success
- Fallback: If interventions not valued, reassess organizational context and coordination needs

**Scoping Decision:** Phase 3 multi-user features optional. Tool delivers value as personal utility even if Trojan Horse adoption strategy fails.

**Risk 4: Privacy Concerns Block Access to Contribution Data**

**Mitigation Strategy:**
- Only use publicly available GitLab data (all contribution data visible to authenticated users)
- If concerns raised, emphasize transparency: "It's just GitLab API queries, nothing hidden"
- Contingency: Add explicit consent/opt-out mechanism if privacy pushback occurs
- Fallback: Use only YOUR OWN contribution data (personal awareness tool only)

**Scoping Decision:** No privacy controls in Phase 1. Address only if concerns arise during validation. All data already visible in GitLab UI.

### Scope Flexibility & Adaptation

**Flexible Elements (Can adjust during development):**

- **Feature sequence:** If collision detection proves more valuable than expertise discovery in Week 2, reorder priorities
- **Algorithm complexity:** Start simple, add sophistication only if validation shows need
- **Update frequency:** Begin with 1-minute batch updates, optimize to real-time only if users demand it
- **Visualization approach:** Heatmaps vs. tables vs. graphs - pick what feels natural during implementation
- **Phase 2 feature selection:** Choose 1-2 advanced features based on Phase 1 learnings, not all three

**Rigid Elements (Cannot change without invalidating strategy):**

- **Week 8 validation gate:** Must assess personal value at 8 weeks. No "just a bit longer" extensions.
- **Week 16 demand validation:** Must have organic access requests by Week 16 or accept personal-only path
- **No ML/LLM in Phase 1:** Violating this adds complexity that undermines validation-gated approach
- **Performance budget (\<500ms, \<1s search):** Non-negotiable for developer tool. Slow = abandoned.
- **Keyboard-first navigation:** Core UX principle for engineering audience. Mouse-required = failure.

**Adaptation Triggers (Signals to adjust scope):**

**Expand Scope If:**
- Week 4 validation: Personal usage 7 days/week, interventions exceeding targets → Accelerate Phase 2 features
- Week 10: Multiple colleagues asking for access unprompted → Start multi-user features early
- Week 8: Specific feature delivering 80% of value → Double down on that feature, defer others

**Contract Scope If:**
- Week 4: Personal usage \<3 days/week → Focus on one feature, kill rest
- Week 8: Time investment exceeding 20 hrs/week → Simplify features or accept slower pace
- Week 12: No colleague recognition of effectiveness → Scale back to personal awareness tool only

**Pivot Scope If:**
- GitLab API insufficient by Week 2 → Consider GitHub/Jira integration or different data source
- Interventions not valued but tool informative → Pivot from brokerage to personal learning tool
- Different use case emerges (e.g., onboarding) → Adapt features to actual demand pattern

**Kill Project If:**
- Week 8: No personal value and tool feels like chore
- Week 16: No personal value, no demand, diminishing utility
- Any phase: Time investment unsustainable (>20 hrs/week ongoing)

______________________________________________________________________

## Functional Requirements

### 1. Expertise Discovery & Contributor Search

- **FR1:** Users can search for experts by file path pattern (e.g., "auth/*", "src/api.ts")
- **FR2:** Users can search for experts by module or directory name
- **FR3:** Users can search for experts by topic keywords
- **FR4:** Users can view ranked contributor lists showing expertise scores for a given area
- **FR5:** Users can view detailed expertise profiles showing commit count, issue count, and MR count by area
- **FR6:** Users can see contribution recency indicators (last contribution timestamp)
- **FR7:** Users can visualize code ownership concentration via expertise heatmap
- **FR8:** Users can see expertise scoring breakdown (contribution weighting formula)
- **FR9:** Users can identify the primary maintainer for any file or module

### 2. Decision Archaeology & Historical Context

- **FR10:** Users can perform full-text search across GitLab issues and MR discussions
- **FR11:** Users can view discussion timeline showing evolution of conversations over time
- **FR12:** Users can see extracted decisions and resolutions highlighted in discussions
- **FR13:** Users can discover related discussions grouped by keyword overlap and shared participants
- **FR14:** Users can filter decision archaeology results by project, date range, or participants
- **FR15:** Users can view the context and rationale for past architectural decisions
- **FR16:** Users can trace decision history back to original discussions

### 3. Code Territory Awareness & Collision Detection

- **FR17:** Users can view real-time tracking of open MRs organized by file path
- **FR18:** Users can view recent commit activity organized by file path and author
- **FR19:** Users can see "who's working where" visualization showing active development areas
- **FR20:** Users can receive collision alerts when multiple developers have open MRs touching the same files
- **FR21:** Users can view historical territory ownership showing primary contributors by file over time
- **FR22:** Users can see territory concentration metrics (percentage of commits by top contributor)
- **FR23:** Users can identify potential merge conflicts before they occur
- **FR24:** Users can filter territory views by project, time window, or contributor

### 4. Activity Monitoring & Summaries

- **FR25:** Users can view daily activity digests aggregating commits, issues, and MRs by person
- **FR26:** Users can view daily activity digests aggregating commits, issues, and MRs by team
- **FR27:** Users can see stalled work detection for high-priority issues with no recent activity
- **FR28:** Users can configure stalled work thresholds (days without activity)
- **FR29:** Users can view weekly activity rollups showing trends in commit count, issue velocity, and review participation
- **FR30:** Users can identify activity gaps (periods of no updates) for tracked work items
- **FR31:** Users can see team-level activity summary views
- **FR32:** Users can track activity across multiple GitLab projects simultaneously

### 5. User Authentication & Access Control

- **FR33:** Users can authenticate using GitLab OAuth
- **FR34:** System can access GitLab contribution data (commits, issues, MRs) via authenticated API
- **FR35:** System can maintain user session state across page refreshes
- **FR36:** Users can sign out and revoke GitLab access
- **FR37:** System can enforce data access permissions matching GitLab visibility rules

### 6. User Interface & Navigation

- **FR38:** Users can navigate all features using keyboard shortcuts (keyboard-first design)
- **FR39:** Users can access all interactive elements via tab navigation
- **FR40:** Users can see clear focus indicators for keyboard navigation
- **FR41:** Users can use search interfaces with typeahead/autocomplete
- **FR42:** Users can view data visualizations (heatmaps, timelines, activity graphs)
- **FR43:** Users can filter and sort result sets (expertise rankings, discussion timelines, activity summaries)
- **FR44:** Users can bookmark or save frequently used queries
- **FR45:** Users can access help documentation for each feature

### 7. Data Integration & Real-Time Updates

- **FR46:** System can poll GitLab API for contribution data (commits, issues, MRs)
- **FR47:** System can process and cache expertise calculations for performance
- **FR48:** System can provide real-time updates for territory radar changes via WebSocket
- **FR49:** System can aggregate GitLab events into time-windowed summaries
- **FR50:** System can detect and extract decision-indicating patterns from discussion text
- **FR51:** System can calculate expertise scores using contribution weighting algorithm
- **FR52:** System can track and update territory ownership as new contributions occur
- **FR53:** System can maintain historical data for trend analysis and evolution views

### 8. Cross-Feature Integration & Context Switching

- **FR54:** Users can view an expert's recent activity summary directly from expertise search results
- **FR55:** Users can view historical decisions related to specific files or modules from territory views
- **FR56:** Users can search for experts on specific topics mentioned in activity summaries
- **FR57:** Users can view current code ownership and active work for files mentioned in historical decisions
- **FR58:** System can resolve cross-references between expertise data, territory data, and decision history using shared identifiers (user ID, file path, project)

### 9. User Preferences & Configuration

- **FR59:** Users can configure collision alert sensitivity (same file only, same module, same project)
- **FR60:** Users can configure stalled work detection threshold (3 days, 5 days, 1 week)
- **FR61:** Users can enable/disable specific alert types (collisions, stalled work, high-priority updates)
- **FR62:** Users can configure notification delivery method (in-app only, browser notifications)

### 10. Knowledge Gap Discovery & Onboarding

- **FR63:** Users can identify areas of the codebase where they have low or zero contribution history
- **FR64:** Users can see which team members have expertise in their knowledge gap areas
- **FR65:** Users can view decision history for areas they haven't worked in yet
- **FR66:** Users can identify team members with limited contribution history in specific modules
- **FR67:** Users can see onboarding progress based on expanding contribution footprint
- **FR68:** System can identify modules where a user has below-threshold contribution activity

______________________________________________________________________

